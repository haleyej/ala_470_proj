{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "documentary-tournament",
   "metadata": {},
   "source": [
    "<h1>ALA 470 Final Project</h1>\n",
    "</br>\n",
    "By Haley Johnson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "desirable-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjacent-orange",
   "metadata": {},
   "source": [
    "<h2>Load Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "victorian-mills",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake.csv\")\n",
    "df_2 = pd.read_csv(\"news_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nominated-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as s:\n",
    "    stopwords = s.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continued-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "buried-brief",
   "metadata": {},
   "source": [
    "<h2>Clean Up</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "martial-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT IF YOU WANT TO USE BOTH FILES\n",
    "# df_1['title'] = df_1['title'].str.lower()\n",
    "# df_1['author'] = df_1['author'].str.lower()\n",
    "\n",
    "# df_1 = df_1.drop(columns = {'language', 'site_url', 'main_img_url', 'type', 'published', 'text'}, axis = 1)\n",
    "\n",
    "# df_2['title'] = df_2['title'].str.lower()\n",
    "# df_2['author'] = df_2['author'].str.lower()\n",
    "\n",
    "# df = df_1.merge(df_2, on = ['title', 'author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-graphics",
   "metadata": {},
   "source": [
    "<h2>NLTK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-liechtenstein",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "gross-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "specified-professor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text_normalized'] = df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pointed-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text_normalized'].apply(lambda t: [word for sent in nltk.sent_tokenize(t) for word in nltk.word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "collective-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['non_stopwords'] = df['tokens'].apply(lambda x: [w for w in x if w not in stopwords])\n",
    "df['non_stopwords_count'] = df['non_stopwords'].apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-teens",
   "metadata": {},
   "source": [
    "<h3>Top Tokens by Type</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "sized-swiss",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "biblical-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = []\n",
    "for i in range(len(types)):\n",
    "    matches = df[df['type'] == types[i]]\n",
    "    exploded_tokens = matches.explode('non_stopwords')\n",
    "    exploded_tokens = exploded_tokens[pd.isnull(exploded_tokens['non_stopwords']) == False]\n",
    "    top_tokens.append(exploded_tokens['non_stopwords'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-procedure",
   "metadata": {},
   "source": [
    "<h3>Filter Out Puncutation Tokens</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "worthy-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\"'\", \"'\", '\"', '\"', \".\", \"?\", \",\", \"!\", \"-\", \",\", \".\", '”', '“', \";\", \":\", \"(\", \")\", \"’\",\n",
    "               '–', \"&\", '``', \"''\", \"'s\", \"...\", \"http\", \"www\", \"https\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dominican-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(top_tokens)):\n",
    "    current = top_tokens[i].reset_index().rename(columns = {'index': 'token', 'non_stopwords': 'count'} )\n",
    "    current = current[current['token'].apply(lambda s: s not in punctuation) == True]\n",
    "    top_tokens[i] = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authentic-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = wub['count'], y = wub['token']).set(title = types[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-warrant",
   "metadata": {},
   "source": [
    "<h2>Visualize</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pending-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, sharex = True)\n",
    "fig.set_size_inches(25, 10)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i == 7:\n",
    "        break\n",
    "    top = top_tokens[i].sort_values(by = 'count', ascending = False)[:10]\n",
    "    grid = sns.barplot(x = top['count'], y = top['token'], ax = ax)\n",
    "    grid.set(title = types[i], xlabel = 'Token', ylabel = 'Frequency')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yellow-civilian",
   "metadata": {},
   "source": [
    "<h3>Named Entity Recognition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "searching-rouge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_entities(s):\n",
    "    '''Takes in column with list \n",
    "        of non stopwords'''\n",
    "    entities = []\n",
    "    targets = ['PERSON', 'GPE', 'ORGANIZATION', 'FACILITY', 'NORP', \n",
    "               'ORG', 'LOC', 'EVENT', 'LAW', 'WORK_OF_ART']\n",
    "    tagged = nltk.pos_tag(s)\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "    for entity in entities.subtrees():\n",
    "        if entity.label() in targets:\n",
    "            matches = []\n",
    "            for leaf in entity.leaves():\n",
    "                matches.append(leaf[0])\n",
    "            entities.append(\" \".join(matches))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "sticky-trout",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_entities(s):\n",
    "    people = []\n",
    "    s = str(s)\n",
    "    tokens = [word for sent in nltk.sent_tokenize(s) for word in nltk.word_tokenize(sent)]\n",
    "    targets = ['PERSON', 'GPE', 'ORGANIZATION', 'FACILITY', 'NORP', 'ORG', \n",
    "               'LOC', 'EVENT', 'LAW', 'WORK_OF_ART']\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "    for entity in entities.subtrees():\n",
    "        if entity.label() == \"PERSON\":\n",
    "            name = []\n",
    "            for leaf in entity.leaves():\n",
    "                name.append(leaf[0])\n",
    "            people.append(\" \".join(name))\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-observer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'] = df['non_stopwords'].apply(get_target_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-fabric",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-correlation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Persons'] = df['text_normalized'].apply(get_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-deficit",
   "metadata": {},
   "source": [
    "<h2>Visualize</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-society",
   "metadata": {},
   "outputs": [],
   "source": [
    "top = []\n",
    "for i in range(len(types)):\n",
    "    matches = df[df['type'] == types[i]]\n",
    "    exploded = matches.explode('Persons')\n",
    "    exploded = exploded[pd.isnull(exploded['Persons']) == False]\n",
    "    top.append(exploded['Persons'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = top[2]\n",
    "top = top[:2] + top[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4)\n",
    "fig.set_size_inches(25, 10)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i == 7:\n",
    "        break\n",
    "    t = types[i]\n",
    "    grid = sns.barplot(x = top[i].values, y = top[i].index, ax = ax)\n",
    "    grid.set(ylabel = t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-muslim",
   "metadata": {},
   "outputs": [],
   "source": [
    "top[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-madonna",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "economic-repository",
   "metadata": {},
   "source": [
    "<h1>ALA 470 Final Project</h1>\n",
    "\n",
    "<p>By Haley Johnson</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "liberal-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "current-willow",
   "metadata": {},
   "source": [
    "<h2>Load Data</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hourly-elements",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"fake.csv\")\n",
    "df_2 = pd.read_csv(\"news_articles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "silver-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stopwords.txt') as s:\n",
    "    stopwords = s.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "least-hamburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = stopwords.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-judgment",
   "metadata": {},
   "source": [
    "<h2>Clean Up</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "capital-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNCOMMENT IF YOU WANT TO USE BOTH FILES\n",
    "# df_1['title'] = df_1['title'].str.lower()\n",
    "# df_1['author'] = df_1['author'].str.lower()\n",
    "\n",
    "# df_1 = df_1.drop(columns = {'language', 'site_url', 'main_img_url', 'type', 'published', 'text'}, axis = 1)\n",
    "\n",
    "# df_2['title'] = df_2['title'].str.lower()\n",
    "# df_2['author'] = df_2['author'].str.lower()\n",
    "\n",
    "# df = df_1.merge(df_2, on = ['title', 'author'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-baltimore",
   "metadata": {},
   "source": [
    "<h2>NLTK</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-spine",
   "metadata": {},
   "source": [
    "<h3>Tokenize</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "listed-harvey",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str)\n",
    "df['text_normalized'] = df['text'].str.lower()\n",
    "df['tokens'] = df['text_normalized'].apply(lambda t: [word for sent in nltk.sent_tokenize(t) for word in nltk.word_tokenize(sent)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "regulated-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['non_stopwords'] = df['tokens'].apply(lambda x: [w for w in x if w not in stopwords])\n",
    "df['non_stopwords_count'] = df['non_stopwords'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "immediate-metropolitan",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].astype(str)\n",
    "df['title_normalized'] = df['title'].str.lower()\n",
    "df['title_tokens'] = df['title_normalized'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_no_stopwords'] = df['title_tokens'].apply(lambda t: [w for w in t if w not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-correspondence",
   "metadata": {},
   "source": [
    "<h3>Top Tokens by Type</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "north-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = list(df['type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "natural-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_tokens = []\n",
    "for i in range(len(types)):\n",
    "    matches = df[df['type'] == types[i]]\n",
    "    exploded_tokens = matches.explode('non_stopwords')\n",
    "    exploded_tokens = exploded_tokens[pd.isnull(exploded_tokens['non_stopwords']) == False]\n",
    "    top_tokens.append(exploded_tokens['non_stopwords'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-internship",
   "metadata": {},
   "source": [
    "<h3>Filter Out Puncutation Tokens</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "infrared-louisiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\"'\", \"'\", '\"', '\"', \".\", \"?\", \",\", \"!\", \"-\", \",\", \".\", '”', '“', \";\", \":\", \"(\", \")\", \"’\",\n",
    "               '–', \"&\", '``', \"''\", \"'s\", \"...\", \"http\", \"www\", \"https\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "driven-technology",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(top_tokens)):\n",
    "    current = top_tokens[i].reset_index().rename(columns = {'index': 'token', 'non_stopwords': 'count'} )\n",
    "    current = current[current['token'].apply(lambda s: s not in punctuation) == True]\n",
    "    top_tokens[i] = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sticky-insertion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = wub['count'], y = wub['token']).set(title = types[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-motorcycle",
   "metadata": {},
   "source": [
    "<h2>Visualize</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, sharex = True)\n",
    "fig.set_size_inches(25, 10)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i == 7:\n",
    "        break\n",
    "    top = top_tokens[i].sort_values(by = 'count', ascending = False)[:10]\n",
    "    grid = sns.barplot(x = top['count'], y = top['token'], ax = ax)\n",
    "    grid.set(title = types[i], xlabel = 'Token', ylabel = 'Frequency')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-curve",
   "metadata": {},
   "source": [
    "<h3>Named Entity Recognition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "automatic-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_all_entities(s):\n",
    "#     '''Takes in column with list \n",
    "#         of non stopwords'''\n",
    "#     entities = []\n",
    "#     targets = ['PERSON', 'GPE', 'ORGANIZATION', 'FACILITY', 'NORP', \n",
    "#                'ORG', 'LOC', 'EVENT', 'LAW', 'WORK_OF_ART']\n",
    "#     tagged = nltk.pos_tag(s)\n",
    "#     entities = nltk.chunk.ne_chunk(tagged)\n",
    "#     for entity in entities.subtrees():\n",
    "#         if entity.label() in targets:\n",
    "#             matches = []\n",
    "#             for leaf in entity.leaves():\n",
    "#                 matches.append(leaf[0])\n",
    "#             entities.append(\" \".join(matches))\n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "august-ceremony",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_entities(s):\n",
    "    people = []\n",
    "    s = str(s)\n",
    "    tokens = [word for sent in nltk.sent_tokenize(s) for word in nltk.word_tokenize(sent)]\n",
    "    targets = ['PERSON', 'GPE', 'ORGANIZATION', 'FACILITY', 'NORP', 'ORG', \n",
    "               'LOC', 'EVENT', 'LAW', 'WORK_OF_ART']\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    entities = nltk.chunk.ne_chunk(tagged)\n",
    "\n",
    "    for entity in entities.subtrees():\n",
    "        if entity.label() == \"PERSON\":\n",
    "            name = []\n",
    "            for leaf in entity.leaves():\n",
    "                name.append(leaf[0])\n",
    "            people.append(\" \".join(name))\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ruled-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['entities'] = df['non_stopwords'].apply(get_target_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "designed-monaco",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          []\n",
       "1               ['washington]\n",
       "2    ['clinton, 'hard, 'hard]\n",
       "3        ['mueller, 'mueller]\n",
       "4          ['person, 'person]\n",
       "Name: entities, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['entities'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-yeast",
   "metadata": {},
   "source": [
    "<h4>NER On Title</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-quarterly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title_entities'] = df['title_no']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-laser",
   "metadata": {},
   "source": [
    "<h2>Visualize</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reliable-clark",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Persons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Persons'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-eed8fe59fbad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mexploded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Persons'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mexploded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexploded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexploded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Persons'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexploded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Persons'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mexplode\u001b[0;34m(self, column, ignore_index)\u001b[0m\n\u001b[1;32m   7278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7279\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7280\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7281\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7282\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Persons'"
     ]
    }
   ],
   "source": [
    "top = []\n",
    "for i in range(len(types)):\n",
    "    matches = df[df['type'] == types[i]]\n",
    "    exploded = matches.explode('Persons')\n",
    "    exploded = exploded[pd.isnull(exploded['Persons']) == False]\n",
    "    top.append(exploded['Persons'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-soundtrack",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = top[2]\n",
    "top = top[:2] + top[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4)\n",
    "fig.set_size_inches(25, 10)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i == 7:\n",
    "        break\n",
    "    t = types[i]\n",
    "    grid = sns.barplot(x = top[i].values, y = top[i].index, ax = ax)\n",
    "    grid.set(ylabel = t)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-occasions",
   "metadata": {},
   "outputs": [],
   "source": [
    "top[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-baltimore",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
